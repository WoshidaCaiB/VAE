{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#python - 3.6\n",
    "#tensorflow - 1.2\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    \n",
    "    #define weights size, learning rate and input\n",
    "    def __init__(self,**param_size):\n",
    "        self.input_size=param_size.get('input_size')\n",
    "        self.Encoder_layer_1=param_size.get('Encoder_layer_1')\n",
    "        self.Encoder_layer_2=param_size.get('Encoder_layer_2')\n",
    "        self.Decoder_layer_1=param_size.get('Decoder_layer_1')\n",
    "        self.Decoder_layer_2=param_size.get('Decoder_layer_2')\n",
    "        self.z_size=param_size.get('z_size')\n",
    "        self.batch_size=param_size.get('batch_size')\n",
    "        self.weights=self._weight_initialize()\n",
    "        self.X=tf.placeholder(shape=[self.input],dtype=tf.float32)\n",
    "        self.sess=tf.Session()\n",
    "        \n",
    "    #variable intialize method - xaiver\n",
    "    def xavier_init(self,input_size,output_size,scale=1):\n",
    "        bound=scale*np.sqrt(6/(input_size+output_size))\n",
    "        return tf.random_uniform((input_size,output_size),minval=-bound,maxval=bound,dtype=tf.float32)\n",
    "    \n",
    "    #initialize weights, bias. Latent Code - z\n",
    "    #Encode and Decode - 2 layers MLP\n",
    "    def _weight_initialize(self):\n",
    "        self.weights=dict()\n",
    "        self.weights['Encoder']={\n",
    "            'h1':tf.Variable(self.xavier_init(self.input_size,self.Encoder_layer_1))\n",
    "            'h2':tf.Variable(self.xavier_init(self.Encoder_layer_1,self.Encoder_layer_2))\n",
    "            'z_mean':tf.Variable(self.xavier_init(self.Encoder_layer_2,self.z_size))\n",
    "            'z_var':tf.Variable(self.xavier_init(self.Encoder_layer_2,self.z_size))\n",
    "            'bias1':tf.Variable(tf.zeros([self.Encoder_layer_1],dtype=tf.float32))\n",
    "            'bias2':tf.Variable(tf.zeros([self.Encoder_layer_2],dtype=tf.float32))\n",
    "            'z_mean_bias':tf.Variable(tf.zeros([self.z_size],dtype=tf.float32))\n",
    "            'z_var_bias':tf.Variable(tf.zeros([self.z_size],dtype=tf.float32))\n",
    "        }\n",
    "        self.weights['Decoder']={\n",
    "            'h1':tf.Variable(self.xavier_init(self.z_size,self.Decoder_layer_1))\n",
    "            'h2':tf.Variable(self.xavier_init(self.Decoder_layer_1,self.Decoder_layer_2))\n",
    "            'out':tf.Varible(self.xavier_init(self.Decoder_layer_2,self.input_size))\n",
    "            'bias1':tf.Variable(tf.zeros([self.Decoder_layer_1],dtype=tf.float32))\n",
    "            'bias2':tf.Variable(tf.zeros([self.Decoder_layer_2],dtype=tf.float32))\n",
    "            'out_bias':tf.Variable(tf.zeros([self.input_size],dtype=tf.float32))\n",
    "        }\n",
    "        return weights\n",
    "    \n",
    "    #construct Encoder network\n",
    "    def Encoder(self,weights,act=tf.nn.softplus):\n",
    "        layer_1=act(tf.add(tf.matmul(self.X,weights['h1']),weights['bias1']))\n",
    "        layer_2=act(tf.add(tf.matmul(layer_1,weights['h2']),weights['bias2']))\n",
    "        z_mean=act(tf.add(tf.matmul(layer_2,weights['z_mean']),weights['z_mean_bias']))\n",
    "        z_var=act(tf.add(tf.matmul(layer_2,weights['z_var']),weights['z_var_bias']))\n",
    "        return z_mean,z_var\n",
    "    \n",
    "    #construct Decoder network\n",
    "    def Decoder(self,z,weights,act=tf.nn.softplus):\n",
    "        layer_1=act(tf.add(tf.matmul(z,weights['h1']),weights['bias1']))\n",
    "        layer_2=act(tf.add(tf.matmul(layer_1,weights['h2']),weights['bias2']))\n",
    "        x_reconstr=tf.nn.sigmoid(tf.add(tf.matmul(layer_2,weights['out']),weights['out_bias']))\n",
    "        return x_reconstr\n",
    "    \n",
    "    #cosntruct network and cost for training, cost contain 2 parts: 1.KL divergence - between normal Gaussian and latent code, 2.the reconstruction loss\n",
    "    def generate_cost(self,learning_rate,act=tf.nn.softplus):\n",
    "        self.z_mean,self.z_var=self.Encoder(self.weights['Encoder'],act)\n",
    "        eps=tf.random_normal(size=(self.batch_size,self.z_size))\n",
    "        self.z=tf.add(self.z_mean,tf.mul(tf.sqrt(tf.exp(self.z_var)),eps))\n",
    "        self.x_reconstr=self.Decoder(self.z,self.weights['Decoder'],act)\n",
    "        E_loss=-tf.reduce_sum(self.X*tf.log(1e-10+self.x_reconstr)+(1.-self.X)*tf.log(1e-10+(1.-self.x_reconstr)),1)\n",
    "        KL_loss=-0.5*tf.reduce_sum(1+self.z_var-tf.square(self.z_mean)-tf.exp(self.z_var),1)\n",
    "        self.cost=tf.reduce_mean(E_loss+KL_loss)\n",
    "        self.Optimizer=tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "    \n",
    "    #training phase - AdamOptimize \n",
    "    def train(self,x,epoch_number,total_batch,learning_rate=0.01,act=tf.nn.softplus):\n",
    "        self.generate_cost(learning_rate,act)\n",
    "        #saver=tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(epoch_number):\n",
    "            for i in range(total_batch):\n",
    "                curr_loss,_=self.run([self.cost,self.Optimizer],feed_dict={self.X:x[i]})\n",
    "                if i%10==0:\n",
    "                    print('current cost:{}'.format(curr_loss))\n",
    "        #saver.save(self.sess,os.path.join(os.getcwd(),'VAE.ckpt'))            \n",
    "    \n",
    "    #generate picture - inout latent code and generate image \n",
    "    def generate(self,latent=None):\n",
    "        if latent==None:\n",
    "            latent=np.random.normal(size=self.z_size)\n",
    "        self.x_reconstr=self.Decoder(self.z,self.weights['Decoder'],act)\n",
    "        x_re=self.sess.run(self.x_reconstr,feed_dict={self.z:latent})\n",
    "        return x_re\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
